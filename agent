import io
import sys
from flask import Flask, jsonify
from autogen import AssistantAgent, UserProxyAgent, config_list_from_json
from contextlib import contextmanager

app = Flask(__name__)

# @contextmanager
# def capture_output():
#     new_out = io.StringIO()
#     old_out = sys.stdout
#     try:
#         sys.stdout = new_out
#         yield new_out
#     finally:
#         sys.stdout = old_out

# Your existing setup for AutoGen
config_list = [
    {
        'model': 'gpt-4',
        'api_key': 'sk---DTKi9T3BlbkFJqK6J9dBOJdpTvAU5PtuH'  # Replace with your actual API key
    },
]
assistant = AssistantAgent("assistant", llm_config={"config_list": config_list})
user_proxy = UserProxyAgent("user_proxy", code_execution_config={"work_dir": "coding"})

@app.route('/get-response', methods=['GET'])
def get_response():
    # Initiates an automated chat between the two agents to solve the task
     #return jsonify({"output": user_proxy.initiate_chat(
     #   assistant,
     #   message="Plot a chart of NVDA and TESLA stock price change YTD."
    #)})
    user_proxy.initiate_chat(
         assistant,
         message="Plot a chart of NVDA and TESLA stock price change YTD."
    )

    return user_proxy.last_message()

if __name__ == '__main__':
    app.run(debug=True)
pulze_agent.py
import json
import openai
import requests

openai.api_key = "sk----2HIAQ6t7PgEdNPvGCaJeAe6e9Asku25qNbAMcMWo_-_W"  # Supply your key however you choose
openai.api_base = "https://api.pulze.ai/v1"  # Enter Pulze's URL

# Make a GET request to the Flask endpoint
response = requests.get("http://localhost:5000/get-response")
if response.status_code == 200:
    question = response.json().get('content')
    print("Input received from the LLM, parsing now!")
    print(f"The results received was {question}")

 # Set up your custom labels and weights
headers = {
    "Pulze-Labels": json.dumps({"foo": "bar", "group": "standard"}),
    "Pulze-Weights": json.dumps({"cost": 0.5, "quality": 0.5, "latency": 0}),
}

text_response = openai.Completion.create(
  model="pulze-v0",
  prompt=f"What would be a good LLM model to answer this type of question: '{question}'?",
  headers=headers,
)

chat_response = openai.ChatCompletion.create(
  model="pulze-v0",
  messages=[{
    "role": "user",
    "content": "Say Hello World!"
  }],
  headers=headers,
)

print(text_response, chat_response)
